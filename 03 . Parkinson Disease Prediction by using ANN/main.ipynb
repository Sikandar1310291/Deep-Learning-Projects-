{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dace6665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.preprocessing import StandardScaler    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f5e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "845f04b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>MDVP:Fo(Hz)</th>\n",
       "      <th>MDVP:Fhi(Hz)</th>\n",
       "      <th>MDVP:Flo(Hz)</th>\n",
       "      <th>MDVP:Jitter(%)</th>\n",
       "      <th>MDVP:Jitter(Abs)</th>\n",
       "      <th>MDVP:RAP</th>\n",
       "      <th>MDVP:PPQ</th>\n",
       "      <th>Jitter:DDP</th>\n",
       "      <th>MDVP:Shimmer</th>\n",
       "      <th>...</th>\n",
       "      <th>Shimmer:DDA</th>\n",
       "      <th>NHR</th>\n",
       "      <th>HNR</th>\n",
       "      <th>status</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>spread1</th>\n",
       "      <th>spread2</th>\n",
       "      <th>D2</th>\n",
       "      <th>PPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phon_R01_S01_1</td>\n",
       "      <td>119.992</td>\n",
       "      <td>157.302</td>\n",
       "      <td>74.997</td>\n",
       "      <td>0.00784</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.00370</td>\n",
       "      <td>0.00554</td>\n",
       "      <td>0.01109</td>\n",
       "      <td>0.04374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06545</td>\n",
       "      <td>0.02211</td>\n",
       "      <td>21.033</td>\n",
       "      <td>1</td>\n",
       "      <td>0.414783</td>\n",
       "      <td>0.815285</td>\n",
       "      <td>-4.813031</td>\n",
       "      <td>0.266482</td>\n",
       "      <td>2.301442</td>\n",
       "      <td>0.284654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phon_R01_S01_2</td>\n",
       "      <td>122.400</td>\n",
       "      <td>148.650</td>\n",
       "      <td>113.819</td>\n",
       "      <td>0.00968</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00465</td>\n",
       "      <td>0.00696</td>\n",
       "      <td>0.01394</td>\n",
       "      <td>0.06134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09403</td>\n",
       "      <td>0.01929</td>\n",
       "      <td>19.085</td>\n",
       "      <td>1</td>\n",
       "      <td>0.458359</td>\n",
       "      <td>0.819521</td>\n",
       "      <td>-4.075192</td>\n",
       "      <td>0.335590</td>\n",
       "      <td>2.486855</td>\n",
       "      <td>0.368674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phon_R01_S01_3</td>\n",
       "      <td>116.682</td>\n",
       "      <td>131.111</td>\n",
       "      <td>111.555</td>\n",
       "      <td>0.01050</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00544</td>\n",
       "      <td>0.00781</td>\n",
       "      <td>0.01633</td>\n",
       "      <td>0.05233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08270</td>\n",
       "      <td>0.01309</td>\n",
       "      <td>20.651</td>\n",
       "      <td>1</td>\n",
       "      <td>0.429895</td>\n",
       "      <td>0.825288</td>\n",
       "      <td>-4.443179</td>\n",
       "      <td>0.311173</td>\n",
       "      <td>2.342259</td>\n",
       "      <td>0.332634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phon_R01_S01_4</td>\n",
       "      <td>116.676</td>\n",
       "      <td>137.871</td>\n",
       "      <td>111.366</td>\n",
       "      <td>0.00997</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00502</td>\n",
       "      <td>0.00698</td>\n",
       "      <td>0.01505</td>\n",
       "      <td>0.05492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08771</td>\n",
       "      <td>0.01353</td>\n",
       "      <td>20.644</td>\n",
       "      <td>1</td>\n",
       "      <td>0.434969</td>\n",
       "      <td>0.819235</td>\n",
       "      <td>-4.117501</td>\n",
       "      <td>0.334147</td>\n",
       "      <td>2.405554</td>\n",
       "      <td>0.368975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phon_R01_S01_5</td>\n",
       "      <td>116.014</td>\n",
       "      <td>141.781</td>\n",
       "      <td>110.655</td>\n",
       "      <td>0.01284</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>0.00655</td>\n",
       "      <td>0.00908</td>\n",
       "      <td>0.01966</td>\n",
       "      <td>0.06425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10470</td>\n",
       "      <td>0.01767</td>\n",
       "      <td>19.649</td>\n",
       "      <td>1</td>\n",
       "      <td>0.417356</td>\n",
       "      <td>0.823484</td>\n",
       "      <td>-3.747787</td>\n",
       "      <td>0.234513</td>\n",
       "      <td>2.332180</td>\n",
       "      <td>0.410335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  MDVP:Fo(Hz)  MDVP:Fhi(Hz)  MDVP:Flo(Hz)  MDVP:Jitter(%)  \\\n",
       "0  phon_R01_S01_1      119.992       157.302        74.997         0.00784   \n",
       "1  phon_R01_S01_2      122.400       148.650       113.819         0.00968   \n",
       "2  phon_R01_S01_3      116.682       131.111       111.555         0.01050   \n",
       "3  phon_R01_S01_4      116.676       137.871       111.366         0.00997   \n",
       "4  phon_R01_S01_5      116.014       141.781       110.655         0.01284   \n",
       "\n",
       "   MDVP:Jitter(Abs)  MDVP:RAP  MDVP:PPQ  Jitter:DDP  MDVP:Shimmer  ...  \\\n",
       "0           0.00007   0.00370   0.00554     0.01109       0.04374  ...   \n",
       "1           0.00008   0.00465   0.00696     0.01394       0.06134  ...   \n",
       "2           0.00009   0.00544   0.00781     0.01633       0.05233  ...   \n",
       "3           0.00009   0.00502   0.00698     0.01505       0.05492  ...   \n",
       "4           0.00011   0.00655   0.00908     0.01966       0.06425  ...   \n",
       "\n",
       "   Shimmer:DDA      NHR     HNR  status      RPDE       DFA   spread1  \\\n",
       "0      0.06545  0.02211  21.033       1  0.414783  0.815285 -4.813031   \n",
       "1      0.09403  0.01929  19.085       1  0.458359  0.819521 -4.075192   \n",
       "2      0.08270  0.01309  20.651       1  0.429895  0.825288 -4.443179   \n",
       "3      0.08771  0.01353  20.644       1  0.434969  0.819235 -4.117501   \n",
       "4      0.10470  0.01767  19.649       1  0.417356  0.823484 -3.747787   \n",
       "\n",
       "    spread2        D2       PPE  \n",
       "0  0.266482  2.301442  0.284654  \n",
       "1  0.335590  2.486855  0.368674  \n",
       "2  0.311173  2.342259  0.332634  \n",
       "3  0.334147  2.405554  0.368975  \n",
       "4  0.234513  2.332180  0.410335  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"parkinsons.data\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6887ea74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195, 24)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6143fdc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name                0\n",
       "MDVP:Fo(Hz)         0\n",
       "MDVP:Fhi(Hz)        0\n",
       "MDVP:Flo(Hz)        0\n",
       "MDVP:Jitter(%)      0\n",
       "MDVP:Jitter(Abs)    0\n",
       "MDVP:RAP            0\n",
       "MDVP:PPQ            0\n",
       "Jitter:DDP          0\n",
       "MDVP:Shimmer        0\n",
       "MDVP:Shimmer(dB)    0\n",
       "Shimmer:APQ3        0\n",
       "Shimmer:APQ5        0\n",
       "MDVP:APQ            0\n",
       "Shimmer:DDA         0\n",
       "NHR                 0\n",
       "HNR                 0\n",
       "status              0\n",
       "RPDE                0\n",
       "DFA                 0\n",
       "spread1             0\n",
       "spread2             0\n",
       "D2                  0\n",
       "PPE                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ba66713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 195 entries, 0 to 194\n",
      "Data columns (total 24 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   name              195 non-null    object \n",
      " 1   MDVP:Fo(Hz)       195 non-null    float64\n",
      " 2   MDVP:Fhi(Hz)      195 non-null    float64\n",
      " 3   MDVP:Flo(Hz)      195 non-null    float64\n",
      " 4   MDVP:Jitter(%)    195 non-null    float64\n",
      " 5   MDVP:Jitter(Abs)  195 non-null    float64\n",
      " 6   MDVP:RAP          195 non-null    float64\n",
      " 7   MDVP:PPQ          195 non-null    float64\n",
      " 8   Jitter:DDP        195 non-null    float64\n",
      " 9   MDVP:Shimmer      195 non-null    float64\n",
      " 10  MDVP:Shimmer(dB)  195 non-null    float64\n",
      " 11  Shimmer:APQ3      195 non-null    float64\n",
      " 12  Shimmer:APQ5      195 non-null    float64\n",
      " 13  MDVP:APQ          195 non-null    float64\n",
      " 14  Shimmer:DDA       195 non-null    float64\n",
      " 15  NHR               195 non-null    float64\n",
      " 16  HNR               195 non-null    float64\n",
      " 17  status            195 non-null    int64  \n",
      " 18  RPDE              195 non-null    float64\n",
      " 19  DFA               195 non-null    float64\n",
      " 20  spread1           195 non-null    float64\n",
      " 21  spread2           195 non-null    float64\n",
      " 22  D2                195 non-null    float64\n",
      " 23  PPE               195 non-null    float64\n",
      "dtypes: float64(22), int64(1), object(1)\n",
      "memory usage: 36.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc7e370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop(columns=['name' , 'status']   )\n",
    "y = data['status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "43d33eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_test , y_train , y_test = train_test_split(x , y , test_size = 0.2 , random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee98e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MDVP:Fo(Hz)</th>\n",
       "      <th>MDVP:Fhi(Hz)</th>\n",
       "      <th>MDVP:Flo(Hz)</th>\n",
       "      <th>MDVP:Jitter(%)</th>\n",
       "      <th>MDVP:Jitter(Abs)</th>\n",
       "      <th>MDVP:RAP</th>\n",
       "      <th>MDVP:PPQ</th>\n",
       "      <th>Jitter:DDP</th>\n",
       "      <th>MDVP:Shimmer</th>\n",
       "      <th>MDVP:Shimmer(dB)</th>\n",
       "      <th>...</th>\n",
       "      <th>MDVP:APQ</th>\n",
       "      <th>Shimmer:DDA</th>\n",
       "      <th>NHR</th>\n",
       "      <th>HNR</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>spread1</th>\n",
       "      <th>spread2</th>\n",
       "      <th>D2</th>\n",
       "      <th>PPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>120.552</td>\n",
       "      <td>131.162</td>\n",
       "      <td>113.787</td>\n",
       "      <td>0.00968</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00463</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.01388</td>\n",
       "      <td>0.04701</td>\n",
       "      <td>0.456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03243</td>\n",
       "      <td>0.06985</td>\n",
       "      <td>0.01222</td>\n",
       "      <td>21.378</td>\n",
       "      <td>0.415564</td>\n",
       "      <td>0.825069</td>\n",
       "      <td>-4.242867</td>\n",
       "      <td>0.299111</td>\n",
       "      <td>2.187560</td>\n",
       "      <td>0.357775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>110.453</td>\n",
       "      <td>127.611</td>\n",
       "      <td>105.554</td>\n",
       "      <td>0.00494</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00244</td>\n",
       "      <td>0.00315</td>\n",
       "      <td>0.00731</td>\n",
       "      <td>0.04128</td>\n",
       "      <td>0.379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03316</td>\n",
       "      <td>0.06688</td>\n",
       "      <td>0.02529</td>\n",
       "      <td>17.707</td>\n",
       "      <td>0.653427</td>\n",
       "      <td>0.706687</td>\n",
       "      <td>-5.333619</td>\n",
       "      <td>0.322044</td>\n",
       "      <td>2.631793</td>\n",
       "      <td>0.228319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>138.190</td>\n",
       "      <td>203.522</td>\n",
       "      <td>83.340</td>\n",
       "      <td>0.00704</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.00406</td>\n",
       "      <td>0.00398</td>\n",
       "      <td>0.01218</td>\n",
       "      <td>0.04479</td>\n",
       "      <td>0.441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03220</td>\n",
       "      <td>0.07761</td>\n",
       "      <td>0.01968</td>\n",
       "      <td>18.305</td>\n",
       "      <td>0.538016</td>\n",
       "      <td>0.741480</td>\n",
       "      <td>-5.418787</td>\n",
       "      <td>0.160267</td>\n",
       "      <td>2.090438</td>\n",
       "      <td>0.229892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>260.105</td>\n",
       "      <td>264.919</td>\n",
       "      <td>237.303</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00205</td>\n",
       "      <td>0.00186</td>\n",
       "      <td>0.00616</td>\n",
       "      <td>0.02030</td>\n",
       "      <td>0.197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01367</td>\n",
       "      <td>0.03557</td>\n",
       "      <td>0.00910</td>\n",
       "      <td>21.083</td>\n",
       "      <td>0.440988</td>\n",
       "      <td>0.628058</td>\n",
       "      <td>-7.517934</td>\n",
       "      <td>0.160414</td>\n",
       "      <td>1.881767</td>\n",
       "      <td>0.075587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>180.978</td>\n",
       "      <td>200.125</td>\n",
       "      <td>155.495</td>\n",
       "      <td>0.00406</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00220</td>\n",
       "      <td>0.00244</td>\n",
       "      <td>0.00659</td>\n",
       "      <td>0.03852</td>\n",
       "      <td>0.331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02877</td>\n",
       "      <td>0.06321</td>\n",
       "      <td>0.02782</td>\n",
       "      <td>16.176</td>\n",
       "      <td>0.583574</td>\n",
       "      <td>0.727747</td>\n",
       "      <td>-5.657899</td>\n",
       "      <td>0.315903</td>\n",
       "      <td>3.098256</td>\n",
       "      <td>0.200423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     MDVP:Fo(Hz)  MDVP:Fhi(Hz)  MDVP:Flo(Hz)  MDVP:Jitter(%)  \\\n",
       "5        120.552       131.162       113.787         0.00968   \n",
       "135      110.453       127.611       105.554         0.00494   \n",
       "122      138.190       203.522        83.340         0.00704   \n",
       "167      260.105       264.919       237.303         0.00339   \n",
       "85       180.978       200.125       155.495         0.00406   \n",
       "\n",
       "     MDVP:Jitter(Abs)  MDVP:RAP  MDVP:PPQ  Jitter:DDP  MDVP:Shimmer  \\\n",
       "5             0.00008   0.00463   0.00750     0.01388       0.04701   \n",
       "135           0.00004   0.00244   0.00315     0.00731       0.04128   \n",
       "122           0.00005   0.00406   0.00398     0.01218       0.04479   \n",
       "167           0.00001   0.00205   0.00186     0.00616       0.02030   \n",
       "85            0.00002   0.00220   0.00244     0.00659       0.03852   \n",
       "\n",
       "     MDVP:Shimmer(dB)  ...  MDVP:APQ  Shimmer:DDA      NHR     HNR      RPDE  \\\n",
       "5               0.456  ...   0.03243      0.06985  0.01222  21.378  0.415564   \n",
       "135             0.379  ...   0.03316      0.06688  0.02529  17.707  0.653427   \n",
       "122             0.441  ...   0.03220      0.07761  0.01968  18.305  0.538016   \n",
       "167             0.197  ...   0.01367      0.03557  0.00910  21.083  0.440988   \n",
       "85              0.331  ...   0.02877      0.06321  0.02782  16.176  0.583574   \n",
       "\n",
       "          DFA   spread1   spread2        D2       PPE  \n",
       "5    0.825069 -4.242867  0.299111  2.187560  0.357775  \n",
       "135  0.706687 -5.333619  0.322044  2.631793  0.228319  \n",
       "122  0.741480 -5.418787  0.160267  2.090438  0.229892  \n",
       "167  0.628058 -7.517934  0.160414  1.881767  0.075587  \n",
       "85   0.727747 -5.657899  0.315903  3.098256  0.200423  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab455dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler    \n",
    "ss = StandardScaler()\n",
    "x_train_standardize = ss.fit_transform(x_train)\n",
    "x_test_standardize = ss.transform(x_test)\n",
    "x_train = x_train_standardize\n",
    "x_test = x_test_standardize "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e84d29",
   "metadata": {},
   "source": [
    "FIND THE BEST PARAMETER FOR THE MODEL   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27c0c5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_tuner\\full_model_tuning\\tuner0.json\n",
      "\n",
      "Best Hyperparameters:\n",
      "---------------------\n",
      "Optimizer: nadam\n",
      "Learning Rate: 0.007779351174806387\n",
      "Number of Layers: 3\n",
      "\n",
      "Layer 0:\n",
      "  Units: 128\n",
      "  Activation: softmax\n",
      "  Dropout: 0.15000000000000002\n",
      "  L2: 0.006\n",
      "\n",
      "Layer 1:\n",
      "  Units: 48\n",
      "  Activation: selu\n",
      "  Dropout: 0.0\n",
      "  L2: 0.005\n",
      "\n",
      "Layer 2:\n",
      "  Units: 48\n",
      "  Activation: linear\n",
      "  Dropout: 0.30000000000000004\n",
      "  L2: 0.007\n",
      "\n",
      "Training Configuration:\n",
      "Batch Size: 48\n",
      "Epochs: 80\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LeakyReLU, PReLU, ELU, Activation\n",
    "from tensorflow.keras import optimizers, regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Tune batch_size and epochs HERE\n",
    "    # -----------------------------\n",
    "    # These need to be defined in build_model to be tracked\n",
    "    hp.Int('batch_size', 16, 128, step=16, default=32)\n",
    "    hp.Int('epochs', 10, 100, step=10, default=50)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Tune number of hidden layers\n",
    "    # -----------------------------\n",
    "    num_layers = hp.Int(\"num_layers\", 1, 5, step=1)\n",
    "    for i in range(num_layers):\n",
    "        units = hp.Int(f\"units_{i}\", 16, 256, step=16)\n",
    "        activation = hp.Choice(\n",
    "            f\"activation_{i}\",\n",
    "            [\n",
    "                \"sigmoid\", \"tanh\", \"relu\", \"leaky_relu\", \"prelu\", \"elu\",\n",
    "                \"selu\", \"swish\", \"softmax\", \"gelu\", \"mish\", \"hard_swish\",\n",
    "                \"hard_sigmoid\", \"linear\", \"binary_step\"\n",
    "            ]\n",
    "        )\n",
    "        l2_reg = hp.Float(f\"l2_{i}\", 0.0, 0.01, step=0.001)\n",
    "\n",
    "        model.add(Dense(units=units, kernel_regularizer=regularizers.l2(l2_reg) if l2_reg > 0 else None))\n",
    "\n",
    "        # Apply activation\n",
    "        if activation == \"leaky_relu\":\n",
    "            model.add(LeakyReLU())\n",
    "        elif activation == \"prelu\":\n",
    "            model.add(PReLU())\n",
    "        elif activation == \"elu\":\n",
    "            model.add(ELU())\n",
    "        elif activation == \"swish\":\n",
    "            model.add(Activation(tf.nn.swish))\n",
    "        elif activation == \"gelu\":\n",
    "            model.add(Activation(tf.nn.gelu))\n",
    "        elif activation == \"mish\":\n",
    "            model.add(Activation(lambda x: x * tf.math.tanh(tf.math.softplus(x))))\n",
    "        elif activation == \"hard_swish\":\n",
    "            model.add(Activation(tf.nn.hard_swish))\n",
    "        elif activation == \"binary_step\":\n",
    "            model.add(Activation(lambda x: tf.where(x >= 0, 1.0, 0.0)))\n",
    "        else:\n",
    "            model.add(Activation(activation))\n",
    "\n",
    "        dropout_rate = hp.Float(f\"dropout_{i}\", 0.0, 0.5, step=0.05)\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    # -----------------------------\n",
    "    # Tune optimizer\n",
    "    # -----------------------------\n",
    "    optimizer_name = hp.Choice(\n",
    "        \"optimizer\",\n",
    "        [\"sgd\", \"momentum\", \"nesterov\", \"adagrad\", \"adadelta\",\n",
    "         \"rmsprop\", \"adam\", \"adamax\", \"nadam\", \"adamw\"]\n",
    "    )\n",
    "    lr = hp.Float(\"learning_rate\", 1e-5, 1e-2, sampling=\"log\")\n",
    "\n",
    "    if optimizer_name == \"sgd\":\n",
    "        optimizer = optimizers.SGD(learning_rate=lr)\n",
    "    elif optimizer_name == \"momentum\":\n",
    "        optimizer = optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "    elif optimizer_name == \"nesterov\":\n",
    "        optimizer = optimizers.SGD(learning_rate=lr, momentum=0.9, nesterov=True)\n",
    "    elif optimizer_name == \"adagrad\":\n",
    "        optimizer = optimizers.Adagrad(learning_rate=lr)\n",
    "    elif optimizer_name == \"adadelta\":\n",
    "        optimizer = optimizers.Adadelta(learning_rate=lr)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = optimizers.RMSprop(learning_rate=lr)\n",
    "    elif optimizer_name == \"adam\":\n",
    "        optimizer = optimizers.Adam(learning_rate=lr)\n",
    "    elif optimizer_name == \"adamax\":\n",
    "        optimizer = optimizers.Adamax(learning_rate=lr)\n",
    "    elif optimizer_name == \"nadam\":\n",
    "        optimizer = optimizers.Nadam(learning_rate=lr)\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        optimizer = optimizers.AdamW(learning_rate=lr)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Custom tuner class to use batch_size and epochs from hyperparameters\n",
    "class MyTuner(kt.RandomSearch):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        # Extract batch_size and epochs from the trial's hyperparameters\n",
    "        kwargs['batch_size'] = trial.hyperparameters.get('batch_size')\n",
    "        kwargs['epochs'] = trial.hyperparameters.get('epochs')\n",
    "        return super(MyTuner, self).run_trial(trial, *args, **kwargs)\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize tuner\n",
    "# -----------------------------\n",
    "tuner = MyTuner(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=25,\n",
    "    directory=\"my_tuner\",\n",
    "    project_name=\"full_model_tuning\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Run hyperparameter search\n",
    "# -----------------------------\n",
    "tuner.search(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_test, y_test)\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Print best hyperparameters\n",
    "# -----------------------------\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(\"---------------------\")\n",
    "print(\"Optimizer:\", best_hp.get(\"optimizer\"))\n",
    "print(\"Learning Rate:\", best_hp.get(\"learning_rate\"))\n",
    "print(\"Number of Layers:\", best_hp.get(\"num_layers\"))\n",
    "\n",
    "for i in range(best_hp.get(\"num_layers\")):\n",
    "    print(f\"\\nLayer {i}:\")\n",
    "    print(\"  Units:\", best_hp.get(f\"units_{i}\"))\n",
    "    print(\"  Activation:\", best_hp.get(f\"activation_{i}\"))\n",
    "    print(\"  Dropout:\", best_hp.get(f\"dropout_{i}\"))\n",
    "    print(\"  L2:\", best_hp.get(f\"l2_{i}\"))\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "# Handle case where batch_size and epochs might not exist in saved tuner\n",
    "try:\n",
    "    print(\"Batch Size:\", best_hp.get(\"batch_size\"))\n",
    "except KeyError:\n",
    "    print(\"Batch Size: Not tuned (using default)\")\n",
    "\n",
    "try:\n",
    "    print(\"Epochs:\", best_hp.get(\"epochs\"))\n",
    "except KeyError:\n",
    "    print(\"Epochs: Not tuned (using default)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d823717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,944</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,192</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m2,944\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_8 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │         \u001b[38;5;34m6,192\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │           \u001b[38;5;34m192\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_9 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │         \u001b[38;5;34m2,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │           \u001b[38;5;34m192\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_10 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m49\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,433</span> (48.57 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,433\u001b[0m (48.57 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,985</span> (46.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,985\u001b[0m (46.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense  , Input , Dropout , BatchNormalization , Activation\n",
    "from tensorflow.keras.regularizers import l2 \n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "\n",
    "# Learning rate for the model\n",
    "learning_rate = 0.007779351174806387\n",
    "\n",
    "# Define Optimizer\n",
    "optimizer = Nadam(learning_rate = learning_rate)\n",
    "\n",
    "#Build Model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(22,)))\n",
    "\n",
    "#Layer 0\n",
    "model.add(Dense(128 , kernel_regularizer=l2(0.006)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))\n",
    "model.add(Dropout(0.15000000000000002))\n",
    "\n",
    "#Layer 1 \n",
    "model.add(Dense(48 , kernel_regularizer=l2(0.005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"selu\"))\n",
    "model.add(Dropout(0.0))\n",
    "\n",
    "#Layer 2\n",
    "model.add(Dense(48 , kernel_regularizer=l2(0.007)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"linear\"))\n",
    "model.add(Dropout(0.30000000000000004))\n",
    "\n",
    "#Output Layer\n",
    "model.add(Dense(1 , activation=\"sigmoid\"))\n",
    "\n",
    "#compile model\n",
    "model.compile(optimizer = optimizer , loss=\"binary_crossentropy\" , metrics=[\"accuracy\"])\n",
    "\n",
    "#Summary\n",
    "model.summary() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61331c92",
   "metadata": {},
   "source": [
    "Trained our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fcdff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COnvert DataFrame to Numpy Array\n",
    "import numpy as np\n",
    "x_train = np.array(x_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f73a76c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 246ms/step - accuracy: 0.6282 - loss: 1.5633 - val_accuracy: 0.8205 - val_loss: 1.4795\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.7628 - loss: 1.2767 - val_accuracy: 0.8205 - val_loss: 1.3714\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8141 - loss: 1.2024 - val_accuracy: 0.8205 - val_loss: 1.2717\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8077 - loss: 1.1216 - val_accuracy: 0.8205 - val_loss: 1.1900\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8718 - loss: 0.9247 - val_accuracy: 0.8205 - val_loss: 1.1140\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8718 - loss: 0.9028 - val_accuracy: 0.8205 - val_loss: 1.0453\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8974 - loss: 0.7923 - val_accuracy: 0.8205 - val_loss: 0.9858\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8910 - loss: 0.7615 - val_accuracy: 0.8205 - val_loss: 0.9323\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9231 - loss: 0.6511 - val_accuracy: 0.8205 - val_loss: 0.8908\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9103 - loss: 0.6311 - val_accuracy: 0.8205 - val_loss: 0.8523\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8846 - loss: 0.6810 - val_accuracy: 0.8205 - val_loss: 0.8220\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8974 - loss: 0.5825 - val_accuracy: 0.8205 - val_loss: 0.7944\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9038 - loss: 0.5444 - val_accuracy: 0.8205 - val_loss: 0.7692\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8974 - loss: 0.4991 - val_accuracy: 0.8205 - val_loss: 0.7475\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9167 - loss: 0.4522 - val_accuracy: 0.8205 - val_loss: 0.7278\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8910 - loss: 0.4628 - val_accuracy: 0.8205 - val_loss: 0.7137\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9295 - loss: 0.3824 - val_accuracy: 0.8205 - val_loss: 0.7025\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9038 - loss: 0.4506 - val_accuracy: 0.8205 - val_loss: 0.6880\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8910 - loss: 0.4126 - val_accuracy: 0.8205 - val_loss: 0.6727\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9423 - loss: 0.3406 - val_accuracy: 0.8205 - val_loss: 0.6602\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9038 - loss: 0.3938 - val_accuracy: 0.8205 - val_loss: 0.6505\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9103 - loss: 0.3492 - val_accuracy: 0.8205 - val_loss: 0.6359\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9295 - loss: 0.3319 - val_accuracy: 0.8205 - val_loss: 0.6264\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9423 - loss: 0.3085 - val_accuracy: 0.8205 - val_loss: 0.6172\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9038 - loss: 0.3174 - val_accuracy: 0.8205 - val_loss: 0.6096\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9103 - loss: 0.3453 - val_accuracy: 0.8205 - val_loss: 0.6021\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8974 - loss: 0.3165 - val_accuracy: 0.8205 - val_loss: 0.6026\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9359 - loss: 0.2904 - val_accuracy: 0.8205 - val_loss: 0.5969\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9231 - loss: 0.3341 - val_accuracy: 0.8205 - val_loss: 0.5897\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9487 - loss: 0.2604 - val_accuracy: 0.8205 - val_loss: 0.5883\n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9295 - loss: 0.2807 - val_accuracy: 0.8205 - val_loss: 0.5842\n",
      "Epoch 32/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9423 - loss: 0.2340 - val_accuracy: 0.8205 - val_loss: 0.5805\n",
      "Epoch 33/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9295 - loss: 0.3470 - val_accuracy: 0.8205 - val_loss: 0.5834\n",
      "Epoch 34/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9231 - loss: 0.2669 - val_accuracy: 0.8205 - val_loss: 0.5866\n",
      "Epoch 35/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9551 - loss: 0.2146 - val_accuracy: 0.8205 - val_loss: 0.6009\n",
      "Epoch 36/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9295 - loss: 0.2562 - val_accuracy: 0.8205 - val_loss: 0.5953\n",
      "Epoch 37/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8974 - loss: 0.3354 - val_accuracy: 0.8205 - val_loss: 0.5765\n",
      "Epoch 38/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9167 - loss: 0.3033 - val_accuracy: 0.8205 - val_loss: 0.5787\n",
      "Epoch 39/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9487 - loss: 0.2633 - val_accuracy: 0.8205 - val_loss: 0.5662\n",
      "Epoch 40/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9423 - loss: 0.2362 - val_accuracy: 0.8205 - val_loss: 0.5576\n",
      "Epoch 41/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9423 - loss: 0.2409 - val_accuracy: 0.8205 - val_loss: 0.5587\n",
      "Epoch 42/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9167 - loss: 0.3229 - val_accuracy: 0.8205 - val_loss: 0.5654\n",
      "Epoch 43/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9359 - loss: 0.2234 - val_accuracy: 0.8205 - val_loss: 0.5768\n",
      "Epoch 44/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9615 - loss: 0.2827 - val_accuracy: 0.8205 - val_loss: 0.5600\n",
      "Epoch 45/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9423 - loss: 0.2473 - val_accuracy: 0.8205 - val_loss: 0.5718\n",
      "Epoch 46/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9359 - loss: 0.2309 - val_accuracy: 0.8205 - val_loss: 0.5727\n",
      "Epoch 47/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9359 - loss: 0.2765 - val_accuracy: 0.8205 - val_loss: 0.5418\n",
      "Epoch 48/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9423 - loss: 0.2119 - val_accuracy: 0.8205 - val_loss: 0.5334\n",
      "Epoch 49/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9551 - loss: 0.2247 - val_accuracy: 0.8205 - val_loss: 0.5364\n",
      "Epoch 50/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9359 - loss: 0.2496 - val_accuracy: 0.8205 - val_loss: 0.5348\n",
      "Epoch 51/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9551 - loss: 0.1998 - val_accuracy: 0.8205 - val_loss: 0.5412\n",
      "Epoch 52/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9359 - loss: 0.2113 - val_accuracy: 0.8205 - val_loss: 0.5471\n",
      "Epoch 53/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9423 - loss: 0.2109 - val_accuracy: 0.8205 - val_loss: 0.5563\n",
      "Epoch 54/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9487 - loss: 0.1949 - val_accuracy: 0.8205 - val_loss: 0.5763\n",
      "Epoch 55/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9615 - loss: 0.1764 - val_accuracy: 0.8205 - val_loss: 0.5929\n",
      "Epoch 56/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9295 - loss: 0.2544 - val_accuracy: 0.8205 - val_loss: 0.5878\n",
      "Epoch 57/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9423 - loss: 0.2140 - val_accuracy: 0.8205 - val_loss: 0.5844\n",
      "Epoch 58/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9615 - loss: 0.1953 - val_accuracy: 0.8205 - val_loss: 0.5851\n",
      "Epoch 59/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9423 - loss: 0.2335 - val_accuracy: 0.8205 - val_loss: 0.5528\n",
      "Epoch 60/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9359 - loss: 0.2104 - val_accuracy: 0.8205 - val_loss: 0.5335\n",
      "Epoch 61/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9551 - loss: 0.1991 - val_accuracy: 0.8205 - val_loss: 0.5361\n",
      "Epoch 62/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9615 - loss: 0.1988 - val_accuracy: 0.8205 - val_loss: 0.5469\n",
      "Epoch 63/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9423 - loss: 0.2042 - val_accuracy: 0.8205 - val_loss: 0.5519\n",
      "Epoch 64/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9295 - loss: 0.2485 - val_accuracy: 0.8205 - val_loss: 0.5374\n",
      "Epoch 65/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9423 - loss: 0.1752 - val_accuracy: 0.8205 - val_loss: 0.5771\n",
      "Epoch 66/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9231 - loss: 0.1997 - val_accuracy: 0.8205 - val_loss: 0.6022\n",
      "Epoch 67/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9615 - loss: 0.1877 - val_accuracy: 0.8205 - val_loss: 0.5926\n",
      "Epoch 68/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9295 - loss: 0.2097 - val_accuracy: 0.8205 - val_loss: 0.5989\n",
      "Epoch 69/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9744 - loss: 0.1717 - val_accuracy: 0.8205 - val_loss: 0.6159\n",
      "Epoch 70/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9359 - loss: 0.2744 - val_accuracy: 0.8205 - val_loss: 0.5928\n",
      "Epoch 71/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9359 - loss: 0.2270 - val_accuracy: 0.8205 - val_loss: 0.5860\n",
      "Epoch 72/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9295 - loss: 0.2510 - val_accuracy: 0.8205 - val_loss: 0.5837\n",
      "Epoch 73/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9103 - loss: 0.3358 - val_accuracy: 0.8205 - val_loss: 0.5272\n",
      "Epoch 74/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9231 - loss: 0.2346 - val_accuracy: 0.8205 - val_loss: 0.5401\n",
      "Epoch 75/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9231 - loss: 0.2669 - val_accuracy: 0.8462 - val_loss: 0.5186\n",
      "Epoch 76/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9487 - loss: 0.2079 - val_accuracy: 0.8205 - val_loss: 0.5292\n",
      "Epoch 77/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9615 - loss: 0.1890 - val_accuracy: 0.8205 - val_loss: 0.5063\n",
      "Epoch 78/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9615 - loss: 0.1982 - val_accuracy: 0.8205 - val_loss: 0.5525\n",
      "Epoch 79/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9359 - loss: 0.2084 - val_accuracy: 0.8205 - val_loss: 0.5520\n",
      "Epoch 80/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9487 - loss: 0.2037 - val_accuracy: 0.8205 - val_loss: 0.5552\n",
      "Epoch 81/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9295 - loss: 0.2265 - val_accuracy: 0.8205 - val_loss: 0.5285\n",
      "Epoch 82/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8974 - loss: 0.2867 - val_accuracy: 0.8205 - val_loss: 0.5250\n",
      "Epoch 83/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9679 - loss: 0.1811 - val_accuracy: 0.8205 - val_loss: 0.5359\n",
      "Epoch 84/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9615 - loss: 0.1665 - val_accuracy: 0.8205 - val_loss: 0.5342\n",
      "Epoch 85/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9103 - loss: 0.3138 - val_accuracy: 0.8205 - val_loss: 0.5634\n",
      "Epoch 86/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9487 - loss: 0.2191 - val_accuracy: 0.8205 - val_loss: 0.5570\n",
      "Epoch 87/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9808 - loss: 0.1450 - val_accuracy: 0.8205 - val_loss: 0.5869\n",
      "Epoch 88/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9231 - loss: 0.2351 - val_accuracy: 0.8205 - val_loss: 0.6297\n",
      "Epoch 89/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9295 - loss: 0.2060 - val_accuracy: 0.8205 - val_loss: 0.5399\n",
      "Epoch 90/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9359 - loss: 0.2257 - val_accuracy: 0.8205 - val_loss: 0.5134\n",
      "Epoch 91/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9103 - loss: 0.2694 - val_accuracy: 0.8462 - val_loss: 0.5109\n",
      "Epoch 92/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9423 - loss: 0.2172 - val_accuracy: 0.8205 - val_loss: 0.5257\n",
      "Epoch 93/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9295 - loss: 0.2446 - val_accuracy: 0.8205 - val_loss: 0.5336\n",
      "Epoch 94/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9359 - loss: 0.2317 - val_accuracy: 0.8718 - val_loss: 0.5435\n",
      "Epoch 95/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9551 - loss: 0.1630 - val_accuracy: 0.8718 - val_loss: 0.5509\n",
      "Epoch 96/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9231 - loss: 0.2345 - val_accuracy: 0.8718 - val_loss: 0.5029\n",
      "Epoch 97/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9487 - loss: 0.1622 - val_accuracy: 0.8718 - val_loss: 0.4601\n",
      "Epoch 98/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9359 - loss: 0.2225 - val_accuracy: 0.8718 - val_loss: 0.3978\n",
      "Epoch 99/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9487 - loss: 0.1857 - val_accuracy: 0.8718 - val_loss: 0.3924\n",
      "Epoch 100/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9744 - loss: 0.1520 - val_accuracy: 0.8718 - val_loss: 0.4248\n",
      "Restoring model weights from the end of the best epoch: 99.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x28f9ceb2fd0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor = \"val_loss\",\n",
    "    patience = 30,\n",
    "    restore_best_weights = True ,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data = (x_test , y_test),\n",
    "    epochs = 100,\n",
    "    callbacks = [early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "adb04de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-8.01728725e-01 -7.08305120e-01 -1.06033035e-01  6.82895780e-01\n",
      "   9.98632848e-01  4.29499000e-01  1.35876536e+00  4.28358644e-01\n",
      "   8.52202237e-01  8.20883095e-01  7.00875580e-01  1.33475292e+00\n",
      "   4.43796813e-01  7.01193035e-01 -3.00025582e-01 -1.22185282e-01\n",
      "  -7.62624919e-01  1.88453329e+00  1.35367787e+00  8.98542821e-01\n",
      "  -4.82941955e-01  1.64980972e+00]\n",
      " [-1.04374218e+00 -7.49504328e-01 -2.99199224e-01 -2.25731373e-01\n",
      "  -8.37831646e-02 -2.55150288e-01 -9.59980935e-02 -2.56283075e-01\n",
      "   5.63407123e-01  4.48115528e-01  6.07699811e-01  6.01423979e-01\n",
      "   4.84458447e-01  6.08015954e-01  1.48766174e-03 -8.98580074e-01\n",
      "   1.49469137e+00 -2.82487631e-01  3.73667508e-01  1.17531848e+00\n",
      "   6.51777387e-01  2.68643141e-01]\n",
      " [-3.79049808e-01  1.31225437e-01 -8.20393622e-01  1.76824957e-01\n",
      "   1.86820835e-01  2.51302600e-01  1.81577459e-01  2.51206309e-01\n",
      "   7.40313053e-01  7.48266041e-01  9.44638431e-01  5.81795275e-01\n",
      "   4.30985600e-01  9.44645584e-01 -1.27930030e-01 -7.72106588e-01\n",
      "   3.99442703e-01  3.54409486e-01  2.97146410e-01 -7.77148902e-01\n",
      "  -7.31023967e-01  2.85425484e-01]\n",
      " [ 2.54253435e+00  8.43561769e-01  2.79195237e+00 -5.22856295e-01\n",
      "  -8.95595133e-01 -3.77074152e-01 -5.27410686e-01 -3.76121432e-01\n",
      "  -4.93996501e-01 -4.32971478e-01 -3.73939872e-01 -4.67948675e-01\n",
      "  -6.01151824e-01 -3.74264836e-01 -3.72001201e-01 -1.84576035e-01\n",
      "  -5.21351516e-01 -1.72181702e+00 -1.58887875e+00 -7.75374770e-01\n",
      "  -1.26403940e+00 -1.36085522e+00]\n",
      " [ 6.46326423e-01  9.18129832e-02  8.72537971e-01 -3.94421667e-01\n",
      "  -6.24991179e-01 -3.30180347e-01 -3.33442241e-01 -3.31312299e-01\n",
      "   4.24301565e-01  2.15740949e-01  4.92877126e-01  5.23694277e-01\n",
      "   2.39931569e-01  4.92877960e-01  5.98525032e-02 -1.22237754e+00\n",
      "   8.31787407e-01  1.03022479e-01  8.23109299e-02  1.10120344e+00\n",
      "   1.84327960e+00 -2.89794076e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3e0b691e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Parkinson's Disease Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ma516\\AppData\\Local\\Temp\\ipykernel_14452\\2669824717.py:11: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  pred_class = int(pred_prob > 0.5)\n"
     ]
    }
   ],
   "source": [
    "new_patient = np.array([[\n",
    "-1.04374218, -0.749504328, -0.299199224, -0.225731373,\n",
    "-0.0837831646, -0.255150288, -0.0959980935, -0.256283075,\n",
    "0.563407123, 0.448115528, 0.607699811, 0.601423979,\n",
    "0.484458447, 0.608015954, 0.00148766174, -0.898580074,\n",
    "1.49469137, -0.282487631, 0.373667508, 1.17531848,\n",
    "0.651777387, 0.268643141\n",
    "\n",
    "]])  # 22 features\n",
    "pred_prob = model.predict(new_patient)\n",
    "pred_class = int(pred_prob > 0.5)\n",
    "\n",
    "if pred_class == 1:\n",
    "    print(\"Parkinson's Disease Positive\")\n",
    "else:\n",
    "    print(\"Parkinson's Disease Negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23df1f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save entire model (architecture + weights + optimizer)\n",
    "model.save(\"parkinsons_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3955e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
