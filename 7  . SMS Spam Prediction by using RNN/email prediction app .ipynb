{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725d52bf-16fb-4bdd-b375-6de30702a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65e24110-8303-4a51-9fa6-f2938762bac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"spam.csv\", encoding=\"latin-1\")  # or \"ISO-8859-1\"\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeae38ae-3377-4780-8ee7-497a073ec1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "def clean_text(text):\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "    text = re.sub(r'[â€¢*|â–ºâ–ªâ—]', ' ', text)\n",
    "    text = re.sub(r'[\\r\\n\\t]', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "data['v2'] = data['v2'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "545cd9d3-83c0-416d-a306-7264bb94e5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat', 'ok lar joking wif u oni', 'free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry question std txt rate t c s apply 08452810075over18 s', 'u dun say so early hor u c already then say', 'nah i don t think he goes to usf he lives around here though', 'freemsg hey there darling it s been 3 week s now and no word back i d like some fun you up for it still tb ok xxx std chgs to send a1 50 to rcv', 'even my brother is not like to speak with me they treat me like aids patent', 'as per your request melle melle oru minnaminunginte nurungu vettam has been set as your callertune for all callers press 9 to copy your friends callertune', 'winner as a valued network customer you have been selected to receivea a900 prize reward to claim call 09061701461 claim code kl341 valid 12 hours only', 'had your mobile 11 months or more u r entitled to update to the latest colour mobiles with camera for free call the mobile update co free on 08002986030', 'i m gonna be home soon and i don t want to talk about this stuff anymore tonight k i ve cried enough today', 'six chances to win cash from 100 to 20 000 pounds txt csh11 and send to 87575 cost 150p day 6days 16 tsandcs apply reply hl 4 info', 'urgent you have won a 1 week free membership in our a100 000 prize jackpot txt the word claim to no 81010 t c www dbuk net lccltd pobox 4403ldnw1a7rw18', 'i ve been searching for the right words to thank you for this breather i promise i wont take your help for granted and will fulfil my promise you have been wonderful and a blessing at all times', 'i have a date on sunday with will', 'xxxmobilemovieclub to use your credit click the wap link in the next txt message or click here http wap xxxmobilemovieclub com n qjkgighjjgcbl', 'oh k i m watching here', 'eh u remember how 2 spell his name yes i did he v naughty make until i v wet', 'fine if thataos the way u feel thataos the way its gota b', 'england v macedonia dont miss the goals team news txt ur national team to 87077 eg england to 87077 try wales scotland 4txt i141 20 poboxox36504w45wq 16', 'is that seriously how you spell his name', 'ium going to try for 2 months ha ha only joking', 'so i pay first lar then when is da stock comin', 'aft i finish my lunch then i go str down lor ard 3 smth lor u finish ur lunch already', 'ffffffffff alright no way i can meet up with you sooner', 'just forced myself to eat a slice i m really not hungry tho this sucks mark is getting worried he knows i m sick when i turn down pizza lol', 'lol your always so convincing', 'did you catch the bus are you frying an egg did you make a tea are you eating your mom s left over dinner do you feel my love', 'i m back amp we re packing the car now i ll let you know if there s room', 'ahhh work i vaguely remember that what does it feel like lol', 'wait that s still not all that clear were you not sure about me being sarcastic or that that s why x doesn t want to live with us', 'yeah he got in at 2 and was v apologetic n had fallen out and she was actin like spoilt child and he got caught up in that till 2 but we won t go there not doing too badly cheers you', 'k tell me anything about you', 'for fear of fainting with the of all that housework you just did quick have a cuppa', 'thanks for your subscription to ringtone uk your mobile will be charged a5 month please confirm by replying yes or no if you reply no you will not be charged', 'yup ok i go home look at the timings then i msg i again xuhui going to learn on 2nd may too but her lesson is at 8am', 'oops i ll let you know when my roommate s done', 'i see the letter b on my car', 'anything lor u decide', 'hello how s you and how did saturday go i was just texting to see if you d decided to do anything tomo not that i m trying to invite myself or anything', 'pls go ahead with watts i just wanted to be sure do have a great weekend abiola', 'did i forget to tell you i want you i need you i crave you but most of all i love you my sweet arabian steed mmmmmm yummy', '07732584351 rodger burns msg we tried to call you re your reply to our sms for a free nokia mobile free camcorder please call now 08000930705 for delivery tomorrow', 'who are you seeing', 'great i hope you like your man well endowed i am lt gt inches', 'no calls messages missed calls', 'didn t you get hep b immunisation in nigeria', 'fair enough anything going on', 'yeah hopefully if tyler can t do it i could maybe ask around a bit', 'u don t know how stubborn i am i didn t even want to go to the hospital i kept telling mark i m not a weak sucker hospitals are for weak suckers', 'what you thinked about me first time you saw me in class', 'a gram usually runs like lt gt a half eighth is smarter though and gets you almost a whole second gram for lt gt', 'k fyi x has a ride early tomorrow morning but he s crashing at our place tonight', 'wow i never realized that you were so embarassed by your accomodations i thought you liked it since i was doing the best i could and you always seemed so happy about the cave i m sorry i didn t and don t have more to give i m sorry i offered i m sorry your room was so embarassing', 'sms ac sptv the new jersey devils and the detroit red wings play ice hockey correct or incorrect end reply end sptv', 'do you know what mallika sherawat did yesterday find out now lt url gt', 'congrats 1 year special cinema pass for 2 is yours call 09061209465 now c suprman v matrix3 starwars3 etc all 4 free bx420 ip4 5we 150pm dont miss out', 'sorry i ll call later in meeting', 'tell where you reached', 'yes gauti and sehwag out of odi series', 'your gonna have to pick up a 1 burger for yourself on your way home i can t even move pain is killing me', 'ha ha ha good joke girls are situation seekers', 'its a part of checking iq', 'sorry my roommates took forever it ok if i come by now', 'ok lar i double check wif da hair dresser already he said wun cut v short he said will cut until i look nice', 'as a valued customer i am pleased to advise you that following recent review of your mob no you are awarded with a a1500 bonus prize call 09066364589', 'today is song dedicated day which song will u dedicate for me send this to all ur valuable frnds but first rply me', 'urgent ur awarded a complimentary trip to eurodisinc trav aco entry41 or a1000 to claim txt dis to 87121 18 6 a1 50 morefrmmob shracomorsglsuplt 10 ls1 3aj', 'did you hear about the new divorce barbie it comes with all of ken s stuff', 'i plane to give on this month end', 'wah lucky man then can save money hee', 'finished class where are you', 'hi babe im at home now wanna do something xx', 'k k where are you how did you performed', 'u can call me now', 'i am waiting machan call me once you free', 'thats cool i am a gentleman and will treat you with dignity and respect', 'i like you peoples very much but am very shy pa', 'does not operate after lt gt or what', 'its not the same here still looking for a job how much do ta s earn there', 'sorry i ll call later', 'k did you call me just now ah', 'ok i am on the way to home hi hi', 'you will be in the place of that man', 'yup next stop', 'i call you later don t have network if urgnt sms me', 'for real when u getting on yo i only need 2 more tickets and one more jacket and i m done i already used all my multis', 'yes i started to send requests to make it but pain came back so i m back in bed double coins at the factory too i gotta cash in all my nitros', 'i m really not up to it still tonight babe', 'ela kano il download come wen ur free', 'yeah do donut stand to close tho youull catch something', 'sorry to be a pain is it ok if we meet another night i spent late afternoon in casualty and that means i haven t done any of y stuff42moro and that includes all my time sheets and that sorry', 'smile in pleasure smile in pain smile when trouble pours like rain smile when sum1 hurts u smile becoz someone still loves to see u smiling', 'please call our customer service representative on 0800 169 6031 between 10am 9pm as you have won a guaranteed a1000 cash or a5000 prize', 'havent planning to buy later i check already lido only got 530 show in e afternoon u finish work already', 'your free ringtone is waiting to be collected simply text the password mix to 85069 to verify get usher and britney fml', 'watching telugu movie wat abt u', 'i see when we finish we have loads of loans to pay', 'hi wk been ok on hols now yes on for a bit of a run forgot that i have hairdressers appointment at four so need to get home n shower beforehand does that cause prob for u ham', 'please don t text me anymore i have nothing else to say'] 0      ham\n",
      "1      ham\n",
      "2     spam\n",
      "3      ham\n",
      "4      ham\n",
      "      ... \n",
      "95    spam\n",
      "96     ham\n",
      "97     ham\n",
      "98     ham\n",
      "99     ham\n",
      "Name: v1, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x = data['v2'].iloc[:100].astype(str).tolist()\n",
    "y = data['v1'].iloc[:100]\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9826d11-590a-40b2-b76a-4e68c8c76010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751\n",
      "Maximum review length: 60\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence  import pad_sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x)\n",
    "\n",
    "sequence = tokenizer.texts_to_sequences(x)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "print(vocab_size)\n",
    "\n",
    "import numpy as np\n",
    "max_len = max(len(seq) for seq in sequence)\n",
    "print(\"Maximum review length:\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "022be45f-a154-4491-9a91-139c00c78f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences:\n",
      " [[  0   0   0 ...  76 251 131]\n",
      " [  0   0   0 ... 133   9 252]\n",
      " [  0   0   0 ... 140 263  14]\n",
      " ...\n",
      " [  0   0   0 ... 737   2 188]\n",
      " [  0   0   0 ...   7   9 749]\n",
      " [  0   0   0 ... 751   2 102]]\n",
      "Vocabulary Size: 751\n"
     ]
    }
   ],
   "source": [
    "max_len =60\n",
    "x = pad_sequences(sequence , maxlen=max_len)\n",
    "\n",
    "print(\"Sequences:\\n\", x)\n",
    "print(\"Vocabulary Size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a83302a-d894-4f12-9869-969c69dcc703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y  = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b86b87-2a96-43eb-9738-9e74aea41598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train , x_test , y_train , y_test = train_test_split(x,y,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db9b9ae1-881e-4172-a109-db99a6d2ec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 25 Complete [00h 00m 42s]\n",
      "val_accuracy: 0.10000000149011612\n",
      "\n",
      "Best val_accuracy So Far: 0.949999988079071\n",
      "Total elapsed time: 00h 13m 49s\n",
      "\n",
      "============================================================\n",
      "BEST HYPERPARAMETERS\n",
      "============================================================\n",
      "\n",
      "ğŸ“ Embedding Configuration:\n",
      "  Embedding Dimension: 250\n",
      "\n",
      "ğŸ“Š Optimizer Configuration:\n",
      "  Optimizer: adagrad\n",
      "  Learning Rate: 0.000622\n",
      "\n",
      "ğŸ”„ RNN Architecture:\n",
      "  Number of RNN Layers: 3\n",
      "\n",
      "  RNN Layer 1:\n",
      "    Units: 240\n",
      "    Activation: prelu\n",
      "    Dropout: 0.400\n",
      "    Recurrent Dropout: 0.050\n",
      "    L2 Regularization: 0.0090\n",
      "\n",
      "  RNN Layer 2:\n",
      "    Units: 256\n",
      "    Activation: linear\n",
      "    Dropout: 0.450\n",
      "    Recurrent Dropout: 0.000\n",
      "    L2 Regularization: 0.0070\n",
      "\n",
      "  RNN Layer 3:\n",
      "    Units: 144\n",
      "    Activation: selu\n",
      "    Dropout: 0.100\n",
      "    Recurrent Dropout: 0.250\n",
      "    L2 Regularization: 0.0010\n",
      "\n",
      "ğŸ”— Dense Layers:\n",
      "  Number of Dense Layers: 3\n",
      "\n",
      "  Dense Layer 1:\n",
      "    Units: 144\n",
      "    Activation: leaky_relu\n",
      "    Dropout: 0.400\n",
      "    L2 Regularization: 0.0030\n",
      "\n",
      "  Dense Layer 2:\n",
      "    Units: 160\n",
      "    Activation: hard_sigmoid\n",
      "    Dropout: 0.400\n",
      "    L2 Regularization: 0.0030\n",
      "\n",
      "  Dense Layer 3:\n",
      "    Units: 64\n",
      "    Activation: linear\n",
      "    Dropout: 0.000\n",
      "    L2 Regularization: 0.0070\n",
      "\n",
      "âš™ï¸ Training Configuration:\n",
      "  Batch Size: 80\n",
      "  Epochs: 100\n",
      "\n",
      "============================================================\n",
      "BEST MODEL SUMMARY\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ma516\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adagrad', because it has 2 variables whereas the saved optimizer has 21 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A total of 9 objects could not be loaded. Example error message for object <Embedding name=embedding, built=True>:\n\nThe shape of the target variable and the shape of the target value in `variable.assign(value)` must match. variable.shape=(752, 250), Received: value.shape=(3068, 250). Target variable: <Variable path=sequential/embedding/embeddings, shape=(752, 250), dtype=float32, value=[[-0.0316867   0.03240318 -0.03027389 ... -0.04839934 -0.04408095\n  -0.00155345]\n [-0.02706999 -0.02510197 -0.00506289 ... -0.01302626 -0.03740447\n  -0.04695771]\n [-0.01223681 -0.04356085  0.00931966 ...  0.00909568 -0.01757501\n   0.01180089]\n ...\n [-0.00961591  0.04086367 -0.04490916 ... -0.01616105  0.03990659\n   0.02817952]\n [ 0.04586116  0.00210671 -0.03703226 ...  0.04814288  0.04680561\n   0.02801216]\n [-0.01439121 -0.02962635 -0.01505201 ... -0.00956391 -0.0035878\n   0.04159129]]>\n\nList of objects that could not be loaded:\n[<Embedding name=embedding, built=True>, <SimpleRNNCell name=simple_rnn_cell, built=True>, <PReLU name=p_re_lu, built=True>, <SimpleRNNCell name=simple_rnn_cell, built=True>, <SimpleRNNCell name=simple_rnn_cell, built=True>, <Dense name=dense, built=True>, <Dense name=dense_1, built=True>, <Dense name=dense_2, built=True>, <Dense name=dense_3, built=True>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 252\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBEST MODEL SUMMARY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m--> 252\u001b[0m best_model \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_models(num_models\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    253\u001b[0m best_model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:400\u001b[0m, in \u001b[0;36mTuner.get_best_models\u001b[1;34m(self, num_models)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the best model(s), as determined by the tuner's objective.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03mThe models are loaded with the weights corresponding to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    List of trained model instances sorted from the best to the worst.\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;66;03m# Method only exists in this class for the docstring override.\u001b[39;00m\n\u001b[1;32m--> 400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget_best_models(num_models)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:366\u001b[0m, in \u001b[0;36mBaseTuner.get_best_models\u001b[1;34m(self, num_models)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the best model(s), as determined by the objective.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mThis method is for querying the models trained during the search.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;124;03m    List of trained models sorted from the best to the worst.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    365\u001b[0m best_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_best_trials(num_models)\n\u001b[1;32m--> 366\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model(trial) \u001b[38;5;28;01mfor\u001b[39;00m trial \u001b[38;5;129;01min\u001b[39;00m best_trials]\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m models\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:331\u001b[0m, in \u001b[0;36mTuner.load_model\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# Reload best checkpoint.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# Only load weights to avoid loading `custom_objects`.\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m maybe_distribute(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution_strategy):\n\u001b[1;32m--> 331\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_checkpoint_fname(trial\u001b[38;5;241m.\u001b[39mtrial_id))\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:645\u001b[0m, in \u001b[0;36m_raise_loading_failure\u001b[1;34m(error_msgs, warn_only)\u001b[0m\n\u001b[0;32m    643\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg)\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 645\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: A total of 9 objects could not be loaded. Example error message for object <Embedding name=embedding, built=True>:\n\nThe shape of the target variable and the shape of the target value in `variable.assign(value)` must match. variable.shape=(752, 250), Received: value.shape=(3068, 250). Target variable: <Variable path=sequential/embedding/embeddings, shape=(752, 250), dtype=float32, value=[[-0.0316867   0.03240318 -0.03027389 ... -0.04839934 -0.04408095\n  -0.00155345]\n [-0.02706999 -0.02510197 -0.00506289 ... -0.01302626 -0.03740447\n  -0.04695771]\n [-0.01223681 -0.04356085  0.00931966 ...  0.00909568 -0.01757501\n   0.01180089]\n ...\n [-0.00961591  0.04086367 -0.04490916 ... -0.01616105  0.03990659\n   0.02817952]\n [ 0.04586116  0.00210671 -0.03703226 ...  0.04814288  0.04680561\n   0.02801216]\n [-0.01439121 -0.02962635 -0.01505201 ... -0.00956391 -0.0035878\n   0.04159129]]>\n\nList of objects that could not be loaded:\n[<Embedding name=embedding, built=True>, <SimpleRNNCell name=simple_rnn_cell, built=True>, <PReLU name=p_re_lu, built=True>, <SimpleRNNCell name=simple_rnn_cell, built=True>, <SimpleRNNCell name=simple_rnn_cell, built=True>, <Dense name=dense, built=True>, <Dense name=dense_1, built=True>, <Dense name=dense_2, built=True>, <Dense name=dense_3, built=True>]"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, SimpleRNN, LeakyReLU, PReLU, ELU, Activation, Embedding\n",
    "from tensorflow.keras import optimizers, regularizers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "# ===== DEFINE CUSTOM HARD SWISH FUNCTION =====\n",
    "def hard_swish(x):\n",
    "    \"\"\"Hard Swish activation function\"\"\"\n",
    "    return x * tf.nn.relu6(x + 3) / 6\n",
    "\n",
    "# ===== PAD YOUR SEQUENCES FIRST! =====\n",
    "vocab_size = 3067\n",
    "TIMESTEPS = 20  # Truncate from 720\n",
    "\n",
    "# THIS IS CRITICAL - PAD BEFORE EVERYTHING ELSE!\n",
    "# x_train = pad_sequences(x_train, maxlen=TIMESTEPS, padding='post', truncating='post')\n",
    "# x_test = pad_sequences(x_test, maxlen=TIMESTEPS, padding='post', truncating='post')\n",
    "\n",
    "# print(f\"After padding - x_train shape: {x_train.shape}\")\n",
    "# print(f\"After padding - x_test shape: {x_test.shape}\")\n",
    "\n",
    "\n",
    "# ===== HYPERMODEL CLASS WITH EMBEDDING =====\n",
    "class RNNHyperModel(kt.HyperModel):\n",
    "    def __init__(self, timesteps, vocab_size):\n",
    "        self.timesteps = timesteps\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "\n",
    "        # Tune batch_size and epochs\n",
    "        hp.Int('batch_size', 16, 128, step=16, default=32)\n",
    "        hp.Int('epochs', 10, 100, step=10, default=50)\n",
    "\n",
    "        # ===== EMBEDDING LAYER - CRITICAL FOR TEXT DATA =====\n",
    "        embedding_dim = hp.Int('embedding_dim', 50, 256, step=50)\n",
    "        model.add(Embedding(\n",
    "            input_dim=self.vocab_size + 1,  # +1 for padding token\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=self.timesteps,\n",
    "            mask_zero=True\n",
    "        ))\n",
    "\n",
    "        # Tune number of RNN layers\n",
    "        num_layers = hp.Int(\"num_rnn_layers\", 1, 5, step=1)\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            units = hp.Int(f\"rnn_units_{i}\", 16, 256, step=16)\n",
    "            \n",
    "            activation = hp.Choice(\n",
    "                f\"rnn_activation_{i}\",\n",
    "                [\"tanh\", \"relu\", \"sigmoid\", \"leaky_relu\", \"prelu\", \"elu\",\n",
    "                 \"selu\", \"swish\", \"gelu\", \"mish\", \"hard_swish\",\n",
    "                 \"hard_sigmoid\", \"linear\"]\n",
    "            )\n",
    "            \n",
    "            dropout = hp.Float(f\"rnn_dropout_{i}\", 0.0, 0.5, step=0.05)\n",
    "            recurrent_dropout = hp.Float(f\"rnn_recurrent_dropout_{i}\", 0.0, 0.3, step=0.05)\n",
    "            l2_reg = hp.Float(f\"rnn_l2_{i}\", 0.0, 0.01, step=0.001)\n",
    "            \n",
    "            return_sequences = (i < num_layers - 1)\n",
    "            \n",
    "            # For custom activations, use 'linear' in SimpleRNN\n",
    "            if activation in [\"leaky_relu\", \"prelu\", \"elu\", \"swish\", \"gelu\", \"mish\", \"hard_swish\"]:\n",
    "                rnn_activation = \"linear\"\n",
    "            else:\n",
    "                rnn_activation = activation\n",
    "            \n",
    "            model.add(SimpleRNN(\n",
    "                units=units,\n",
    "                activation=rnn_activation,\n",
    "                dropout=dropout,\n",
    "                recurrent_dropout=recurrent_dropout,\n",
    "                return_sequences=return_sequences,\n",
    "                kernel_regularizer=regularizers.l2(l2_reg) if l2_reg > 0 else None,\n",
    "                recurrent_regularizer=regularizers.l2(l2_reg) if l2_reg > 0 else None\n",
    "            ))\n",
    "            \n",
    "            # Add custom activation layer if needed\n",
    "            if activation == \"leaky_relu\":\n",
    "                model.add(LeakyReLU())\n",
    "            elif activation == \"prelu\":\n",
    "                model.add(PReLU())\n",
    "            elif activation == \"elu\":\n",
    "                model.add(ELU())\n",
    "            elif activation == \"swish\":\n",
    "                model.add(Activation(tf.nn.swish))\n",
    "            elif activation == \"gelu\":\n",
    "                model.add(Activation(tf.nn.gelu))\n",
    "            elif activation == \"mish\":\n",
    "                model.add(Activation(lambda x: x * tf.math.tanh(tf.math.softplus(x))))\n",
    "            elif activation == \"hard_swish\":\n",
    "                model.add(Activation(hard_swish))  # Use custom hard_swish function\n",
    "\n",
    "        # Optional Dense layers after RNN\n",
    "        use_dense = hp.Boolean(\"use_dense_layers\")\n",
    "        if use_dense:\n",
    "            num_dense = hp.Int(\"num_dense_layers\", 1, 3, step=1)\n",
    "            for i in range(num_dense):\n",
    "                dense_units = hp.Int(f\"dense_units_{i}\", 16, 256, step=16)\n",
    "                dense_activation = hp.Choice(\n",
    "                    f\"dense_activation_{i}\",\n",
    "                    [\"sigmoid\", \"tanh\", \"relu\", \"leaky_relu\", \"prelu\", \"elu\",\n",
    "                     \"selu\", \"swish\", \"gelu\", \"hard_sigmoid\", \"linear\"]\n",
    "                )\n",
    "                l2_reg_dense = hp.Float(f\"dense_l2_{i}\", 0.0, 0.01, step=0.001)\n",
    "\n",
    "                model.add(Dense(\n",
    "                    units=dense_units,\n",
    "                    kernel_regularizer=regularizers.l2(l2_reg_dense) if l2_reg_dense > 0 else None\n",
    "                ))\n",
    "\n",
    "                # Apply activation\n",
    "                if dense_activation == \"leaky_relu\":\n",
    "                    model.add(LeakyReLU())\n",
    "                elif dense_activation == \"prelu\":\n",
    "                    model.add(PReLU())\n",
    "                elif dense_activation == \"elu\":\n",
    "                    model.add(ELU())\n",
    "                elif dense_activation == \"swish\":\n",
    "                    model.add(Activation(tf.nn.swish))\n",
    "                elif dense_activation == \"gelu\":\n",
    "                    model.add(Activation(tf.nn.gelu))\n",
    "                else:\n",
    "                    model.add(Activation(dense_activation))\n",
    "\n",
    "                dropout_rate = hp.Float(f\"dense_dropout_{i}\", 0.0, 0.5, step=0.05)\n",
    "                if dropout_rate > 0:\n",
    "                    model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "        # Output layer\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "        # Tune optimizer\n",
    "        optimizer_name = hp.Choice(\n",
    "            \"optimizer\",\n",
    "            [\"sgd\", \"momentum\", \"nesterov\", \"adagrad\", \"adadelta\",\n",
    "             \"rmsprop\", \"adam\", \"adamax\", \"nadam\", \"adamw\"]\n",
    "        )\n",
    "        lr = hp.Float(\"learning_rate\", 1e-5, 1e-2, sampling=\"log\")\n",
    "\n",
    "        if optimizer_name == \"sgd\":\n",
    "            optimizer = optimizers.SGD(learning_rate=lr)\n",
    "        elif optimizer_name == \"momentum\":\n",
    "            optimizer = optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "        elif optimizer_name == \"nesterov\":\n",
    "            optimizer = optimizers.SGD(learning_rate=lr, momentum=0.9, nesterov=True)\n",
    "        elif optimizer_name == \"adagrad\":\n",
    "            optimizer = optimizers.Adagrad(learning_rate=lr)\n",
    "        elif optimizer_name == \"adadelta\":\n",
    "            optimizer = optimizers.Adadelta(learning_rate=lr)\n",
    "        elif optimizer_name == \"rmsprop\":\n",
    "            optimizer = optimizers.RMSprop(learning_rate=lr)\n",
    "        elif optimizer_name == \"adam\":\n",
    "            optimizer = optimizers.Adam(learning_rate=lr)\n",
    "        elif optimizer_name == \"adamax\":\n",
    "            optimizer = optimizers.Adamax(learning_rate=lr)\n",
    "        elif optimizer_name == \"nadam\":\n",
    "            optimizer = optimizers.Nadam(learning_rate=lr)\n",
    "        elif optimizer_name == \"adamw\":\n",
    "            optimizer = optimizers.AdamW(learning_rate=lr)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=\"binary_crossentropy\",\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n",
    "# ===== CUSTOM TUNER CLASS =====\n",
    "class MyRNNTuner(kt.RandomSearch):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        kwargs['batch_size'] = trial.hyperparameters.get('batch_size')\n",
    "        kwargs['epochs'] = trial.hyperparameters.get('epochs')\n",
    "        return super(MyRNNTuner, self).run_trial(trial, *args, **kwargs)\n",
    "\n",
    "\n",
    "# ===== CREATE HYPERMODEL INSTANCE =====\n",
    "hypermodel = RNNHyperModel(timesteps=TIMESTEPS, vocab_size=vocab_size)\n",
    "\n",
    "# ===== INITIALIZE TUNER =====\n",
    "tuner = MyRNNTuner(\n",
    "    hypermodel=hypermodel,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=25,\n",
    "    directory=\"rnn_tuner\",\n",
    "    project_name=\"full_rnn_tuning\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# ===== RUN HYPERPARAMETER SEARCH =====\n",
    "print(\"Starting RNN hyperparameter search...\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Sequence length (TIMESTEPS): {TIMESTEPS}\")\n",
    "print(f\"x_train shape: {x_train.shape}\")  # Should be (samples, 20)\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Test samples: {len(x_test)}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "tuner.search(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===== PRINT BEST HYPERPARAMETERS =====\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST HYPERPARAMETERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nğŸ“ Embedding Configuration:\")\n",
    "print(f\"  Embedding Dimension: {best_hp.get('embedding_dim')}\")\n",
    "\n",
    "print(\"\\nğŸ“Š Optimizer Configuration:\")\n",
    "print(f\"  Optimizer: {best_hp.get('optimizer')}\")\n",
    "print(f\"  Learning Rate: {best_hp.get('learning_rate'):.6f}\")\n",
    "\n",
    "print(\"\\nğŸ”„ RNN Architecture:\")\n",
    "print(f\"  Number of RNN Layers: {best_hp.get('num_rnn_layers')}\")\n",
    "\n",
    "for i in range(best_hp.get(\"num_rnn_layers\")):\n",
    "    print(f\"\\n  RNN Layer {i+1}:\")\n",
    "    print(f\"    Units: {best_hp.get(f'rnn_units_{i}')}\")\n",
    "    print(f\"    Activation: {best_hp.get(f'rnn_activation_{i}')}\")\n",
    "    print(f\"    Dropout: {best_hp.get(f'rnn_dropout_{i}'):.3f}\")\n",
    "    print(f\"    Recurrent Dropout: {best_hp.get(f'rnn_recurrent_dropout_{i}'):.3f}\")\n",
    "    print(f\"    L2 Regularization: {best_hp.get(f'rnn_l2_{i}'):.4f}\")\n",
    "\n",
    "print(\"\\nğŸ”— Dense Layers:\")\n",
    "if best_hp.get(\"use_dense_layers\"):\n",
    "    print(f\"  Number of Dense Layers: {best_hp.get('num_dense_layers')}\")\n",
    "    for i in range(best_hp.get(\"num_dense_layers\")):\n",
    "        print(f\"\\n  Dense Layer {i+1}:\")\n",
    "        print(f\"    Units: {best_hp.get(f'dense_units_{i}')}\")\n",
    "        print(f\"    Activation: {best_hp.get(f'dense_activation_{i}')}\")\n",
    "        print(f\"    Dropout: {best_hp.get(f'dense_dropout_{i}'):.3f}\")\n",
    "        print(f\"    L2 Regularization: {best_hp.get(f'dense_l2_{i}'):.4f}\")\n",
    "else:\n",
    "    print(\"  No additional dense layers used\")\n",
    "\n",
    "print(\"\\nâš™ï¸ Training Configuration:\")\n",
    "print(f\"  Batch Size: {best_hp.get('batch_size')}\")\n",
    "print(f\"  Epochs: {best_hp.get('epochs')}\")\n",
    "\n",
    "# ===== BUILD AND EVALUATE BEST MODEL (FIXED) =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BUILDING BEST MODEL FROM HYPERPARAMETERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build model with best hyperparameters instead of loading\n",
    "best_model = hypermodel.build(best_hp)\n",
    "best_model.summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING BEST MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train the best model\n",
    "history = best_model.fit(\n",
    "    x_train, \n",
    "    y_train,\n",
    "    batch_size=best_hp.get('batch_size'),\n",
    "    epochs=best_hp.get('epochs'),\n",
    "    validation_data=(x_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "test_loss, test_accuracy = best_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Optional: Save the final model\n",
    "best_model.save('best_rnn_model.h5')\n",
    "print(\"\\nâœ… Best model saved as 'best_rnn_model.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4588aec-ffb1-4bc3-b056-da06e75a5bfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BEST HYPERPARAMETERS\n",
      "============================================================\n",
      "\n",
      "ğŸ“ Embedding Configuration:\n",
      "  Embedding Dimension: 250\n",
      "\n",
      "ğŸ“Š Optimizer Configuration:\n",
      "  Optimizer: adagrad\n",
      "  Learning Rate: 0.000622\n",
      "\n",
      "ğŸ”„ RNN Architecture:\n",
      "  Number of RNN Layers: 3\n",
      "\n",
      "  RNN Layer 1:\n",
      "    Units: 240\n",
      "    Activation: prelu\n",
      "    Dropout: 0.400\n",
      "    Recurrent Dropout: 0.050\n",
      "    L2 Regularization: 0.0090\n",
      "\n",
      "  RNN Layer 2:\n",
      "    Units: 256\n",
      "    Activation: linear\n",
      "    Dropout: 0.450\n",
      "    Recurrent Dropout: 0.000\n",
      "    L2 Regularization: 0.0070\n",
      "\n",
      "  RNN Layer 3:\n",
      "    Units: 144\n",
      "    Activation: selu\n",
      "    Dropout: 0.100\n",
      "    Recurrent Dropout: 0.250\n",
      "    L2 Regularization: 0.0010\n",
      "\n",
      "ğŸ”— Dense Layers:\n",
      "  Number of Dense Layers: 3\n",
      "\n",
      "  Dense Layer 1:\n",
      "    Units: 144\n",
      "    Activation: leaky_relu\n",
      "    Dropout: 0.400\n",
      "    L2 Regularization: 0.0030\n",
      "\n",
      "  Dense Layer 2:\n",
      "    Units: 160\n",
      "    Activation: hard_sigmoid\n",
      "    Dropout: 0.400\n",
      "    L2 Regularization: 0.0030\n",
      "\n",
      "  Dense Layer 3:\n",
      "    Units: 64\n",
      "    Activation: linear\n",
      "    Dropout: 0.000\n",
      "    L2 Regularization: 0.0070\n",
      "\n",
      "âš™ï¸ Training Configuration:\n",
      "  Batch Size: 80\n",
      "  Epochs: 100\n",
      "\n",
      "============================================================\n",
      "BUILDING BEST MODEL FROM HYPERPARAMETERS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ma516\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ p_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PReLU</span>)               â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn_3 (\u001b[38;5;33mSimpleRNN\u001b[0m)        â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ p_re_lu_1 (\u001b[38;5;33mPReLU\u001b[0m)               â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn_4 (\u001b[38;5;33mSimpleRNN\u001b[0m)        â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ simple_rnn_5 (\u001b[38;5;33mSimpleRNN\u001b[0m)        â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ activation_2 (\u001b[38;5;33mActivation\u001b[0m)       â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ activation_3 (\u001b[38;5;33mActivation\u001b[0m)       â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING BEST MODEL\n",
      "============================================================\n",
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12s/step - accuracy: 0.7375 - loss: 10.2335 - val_accuracy: 0.9000 - val_loss: 10.2259\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step - accuracy: 0.5750 - loss: 10.4173 - val_accuracy: 0.9000 - val_loss: 10.2165\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458ms/step - accuracy: 0.7125 - loss: 10.2950 - val_accuracy: 0.9000 - val_loss: 10.2074\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470ms/step - accuracy: 0.6625 - loss: 10.3557 - val_accuracy: 0.9000 - val_loss: 10.1966\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446ms/step - accuracy: 0.6375 - loss: 10.3382 - val_accuracy: 0.9000 - val_loss: 10.1895\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453ms/step - accuracy: 0.6875 - loss: 10.3060 - val_accuracy: 0.9000 - val_loss: 10.1840\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step - accuracy: 0.6625 - loss: 10.3619 - val_accuracy: 0.9000 - val_loss: 10.1764\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 425ms/step - accuracy: 0.6875 - loss: 10.3434 - val_accuracy: 0.9000 - val_loss: 10.1689\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 510ms/step - accuracy: 0.8000 - loss: 10.3037 - val_accuracy: 0.9000 - val_loss: 10.1636\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487ms/step - accuracy: 0.8000 - loss: 10.2176 - val_accuracy: 0.9000 - val_loss: 10.1582\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 427ms/step - accuracy: 0.7250 - loss: 10.2969 - val_accuracy: 0.9000 - val_loss: 10.1512\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482ms/step - accuracy: 0.7250 - loss: 10.2891 - val_accuracy: 0.9000 - val_loss: 10.1452\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 560ms/step - accuracy: 0.7625 - loss: 10.2842 - val_accuracy: 0.9000 - val_loss: 10.1396\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 495ms/step - accuracy: 0.7125 - loss: 10.2831 - val_accuracy: 0.9000 - val_loss: 10.1360\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 492ms/step - accuracy: 0.7375 - loss: 10.1958 - val_accuracy: 0.9000 - val_loss: 10.1314\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452ms/step - accuracy: 0.8125 - loss: 10.2132 - val_accuracy: 0.9000 - val_loss: 10.1266\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 477ms/step - accuracy: 0.6750 - loss: 10.3236 - val_accuracy: 0.9000 - val_loss: 10.1215\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 518ms/step - accuracy: 0.8375 - loss: 10.2484 - val_accuracy: 0.9000 - val_loss: 10.1178\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 431ms/step - accuracy: 0.7625 - loss: 10.2154 - val_accuracy: 0.9000 - val_loss: 10.1146\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379ms/step - accuracy: 0.7625 - loss: 10.2823 - val_accuracy: 0.9000 - val_loss: 10.1098\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step - accuracy: 0.7500 - loss: 10.2026 - val_accuracy: 0.9000 - val_loss: 10.1067\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413ms/step - accuracy: 0.8000 - loss: 10.1954 - val_accuracy: 0.9000 - val_loss: 10.1040\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 443ms/step - accuracy: 0.7375 - loss: 10.2536 - val_accuracy: 0.9000 - val_loss: 10.0996\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424ms/step - accuracy: 0.6750 - loss: 10.2878 - val_accuracy: 0.9000 - val_loss: 10.0952\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 434ms/step - accuracy: 0.7250 - loss: 10.3089 - val_accuracy: 0.9000 - val_loss: 10.0926\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 417ms/step - accuracy: 0.7875 - loss: 10.2006 - val_accuracy: 0.9000 - val_loss: 10.0911\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462ms/step - accuracy: 0.8375 - loss: 10.1991 - val_accuracy: 0.9000 - val_loss: 10.0889\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - accuracy: 0.8375 - loss: 10.1835 - val_accuracy: 0.9000 - val_loss: 10.0868\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 386ms/step - accuracy: 0.7875 - loss: 10.2126 - val_accuracy: 0.9000 - val_loss: 10.0839\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - accuracy: 0.8125 - loss: 10.2219 - val_accuracy: 0.9000 - val_loss: 10.0817\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405ms/step - accuracy: 0.7625 - loss: 10.2355 - val_accuracy: 0.9000 - val_loss: 10.0794\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step - accuracy: 0.7750 - loss: 10.2528 - val_accuracy: 0.9000 - val_loss: 10.0771\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381ms/step - accuracy: 0.8250 - loss: 10.1624 - val_accuracy: 0.9000 - val_loss: 10.0751\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step - accuracy: 0.7375 - loss: 10.1896 - val_accuracy: 0.9000 - val_loss: 10.0728\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448ms/step - accuracy: 0.8000 - loss: 10.2552 - val_accuracy: 0.9000 - val_loss: 10.0708\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440ms/step - accuracy: 0.7125 - loss: 10.2866 - val_accuracy: 0.9000 - val_loss: 10.0680\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486ms/step - accuracy: 0.7875 - loss: 10.2513 - val_accuracy: 0.9000 - val_loss: 10.0662\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 525ms/step - accuracy: 0.7875 - loss: 10.2381 - val_accuracy: 0.9000 - val_loss: 10.0648\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500ms/step - accuracy: 0.7500 - loss: 10.2609 - val_accuracy: 0.9000 - val_loss: 10.0638\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483ms/step - accuracy: 0.8125 - loss: 10.2030 - val_accuracy: 0.9000 - val_loss: 10.0621\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - accuracy: 0.7750 - loss: 10.2980 - val_accuracy: 0.9000 - val_loss: 10.0602\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439ms/step - accuracy: 0.8125 - loss: 10.1987 - val_accuracy: 0.9000 - val_loss: 10.0585\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474ms/step - accuracy: 0.7750 - loss: 10.1967 - val_accuracy: 0.9000 - val_loss: 10.0564\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 429ms/step - accuracy: 0.7625 - loss: 10.2194 - val_accuracy: 0.9000 - val_loss: 10.0543\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - accuracy: 0.7875 - loss: 10.2466 - val_accuracy: 0.9000 - val_loss: 10.0525\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439ms/step - accuracy: 0.7875 - loss: 10.1911 - val_accuracy: 0.9000 - val_loss: 10.0504\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 528ms/step - accuracy: 0.7750 - loss: 10.1995 - val_accuracy: 0.9000 - val_loss: 10.0491\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 539ms/step - accuracy: 0.8125 - loss: 10.1860 - val_accuracy: 0.9000 - val_loss: 10.0470\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 517ms/step - accuracy: 0.8000 - loss: 10.2856 - val_accuracy: 0.9000 - val_loss: 10.0459\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 623ms/step - accuracy: 0.8000 - loss: 10.2077 - val_accuracy: 0.9000 - val_loss: 10.0449\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 429ms/step - accuracy: 0.7875 - loss: 10.2535 - val_accuracy: 0.9000 - val_loss: 10.0432\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362ms/step - accuracy: 0.8125 - loss: 10.2175 - val_accuracy: 0.9000 - val_loss: 10.0418\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - accuracy: 0.7875 - loss: 10.1836 - val_accuracy: 0.9000 - val_loss: 10.0405\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365ms/step - accuracy: 0.7875 - loss: 10.2301 - val_accuracy: 0.9000 - val_loss: 10.0394\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step - accuracy: 0.8000 - loss: 10.2283 - val_accuracy: 0.9000 - val_loss: 10.0385\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step - accuracy: 0.7750 - loss: 10.2661 - val_accuracy: 0.9000 - val_loss: 10.0368\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - accuracy: 0.7750 - loss: 10.2341 - val_accuracy: 0.9000 - val_loss: 10.0358\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 431ms/step - accuracy: 0.8000 - loss: 10.2225 - val_accuracy: 0.9000 - val_loss: 10.0350\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458ms/step - accuracy: 0.8250 - loss: 10.1720 - val_accuracy: 0.9000 - val_loss: 10.0337\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 432ms/step - accuracy: 0.7750 - loss: 10.1990 - val_accuracy: 0.9000 - val_loss: 10.0328\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447ms/step - accuracy: 0.8250 - loss: 10.1723 - val_accuracy: 0.9000 - val_loss: 10.0314\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440ms/step - accuracy: 0.8000 - loss: 10.2389 - val_accuracy: 0.9000 - val_loss: 10.0307\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441ms/step - accuracy: 0.8125 - loss: 10.1998 - val_accuracy: 0.9000 - val_loss: 10.0297\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448ms/step - accuracy: 0.7875 - loss: 10.2478 - val_accuracy: 0.9000 - val_loss: 10.0288\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410ms/step - accuracy: 0.8000 - loss: 10.1763 - val_accuracy: 0.9000 - val_loss: 10.0276\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 0.8000 - loss: 10.1982 - val_accuracy: 0.9000 - val_loss: 10.0265\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 428ms/step - accuracy: 0.7625 - loss: 10.1488 - val_accuracy: 0.9000 - val_loss: 10.0252\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493ms/step - accuracy: 0.8125 - loss: 10.1821 - val_accuracy: 0.9000 - val_loss: 10.0245\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - accuracy: 0.8125 - loss: 10.1692 - val_accuracy: 0.9000 - val_loss: 10.0234\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441ms/step - accuracy: 0.7625 - loss: 10.1840 - val_accuracy: 0.9000 - val_loss: 10.0223\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458ms/step - accuracy: 0.8000 - loss: 10.2166 - val_accuracy: 0.9000 - val_loss: 10.0213\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463ms/step - accuracy: 0.7875 - loss: 10.1952 - val_accuracy: 0.9000 - val_loss: 10.0204\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 677ms/step - accuracy: 0.8000 - loss: 10.1851 - val_accuracy: 0.9000 - val_loss: 10.0200\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 571ms/step - accuracy: 0.7500 - loss: 10.2248 - val_accuracy: 0.9000 - val_loss: 10.0189\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 513ms/step - accuracy: 0.8000 - loss: 10.2203 - val_accuracy: 0.9000 - val_loss: 10.0177\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - accuracy: 0.8125 - loss: 10.1975 - val_accuracy: 0.9000 - val_loss: 10.0167\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 495ms/step - accuracy: 0.8250 - loss: 10.1783 - val_accuracy: 0.9000 - val_loss: 10.0159\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488ms/step - accuracy: 0.7625 - loss: 10.2166 - val_accuracy: 0.9000 - val_loss: 10.0146\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474ms/step - accuracy: 0.8000 - loss: 10.2240 - val_accuracy: 0.9000 - val_loss: 10.0141\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 522ms/step - accuracy: 0.8000 - loss: 10.1933 - val_accuracy: 0.9000 - val_loss: 10.0136\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - accuracy: 0.8125 - loss: 10.2140 - val_accuracy: 0.9000 - val_loss: 10.0129\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.8000 - loss: 10.1880 - val_accuracy: 0.9000 - val_loss: 10.0123\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471ms/step - accuracy: 0.8250 - loss: 10.1343 - val_accuracy: 0.9000 - val_loss: 10.0113\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - accuracy: 0.8250 - loss: 10.2142 - val_accuracy: 0.9000 - val_loss: 10.0104\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 527ms/step - accuracy: 0.8125 - loss: 10.2140 - val_accuracy: 0.9000 - val_loss: 10.0098\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474ms/step - accuracy: 0.8000 - loss: 10.1761 - val_accuracy: 0.9000 - val_loss: 10.0092\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 432ms/step - accuracy: 0.8125 - loss: 10.1328 - val_accuracy: 0.9000 - val_loss: 10.0087\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - accuracy: 0.8250 - loss: 10.1574 - val_accuracy: 0.9000 - val_loss: 10.0080\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472ms/step - accuracy: 0.7875 - loss: 10.1500 - val_accuracy: 0.9000 - val_loss: 10.0069\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472ms/step - accuracy: 0.7750 - loss: 10.1177 - val_accuracy: 0.9000 - val_loss: 10.0056\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 535ms/step - accuracy: 0.7875 - loss: 10.1245 - val_accuracy: 0.9000 - val_loss: 10.0045\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 417ms/step - accuracy: 0.8000 - loss: 10.1774 - val_accuracy: 0.9000 - val_loss: 10.0035\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 422ms/step - accuracy: 0.8250 - loss: 10.1962 - val_accuracy: 0.9000 - val_loss: 10.0025\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step - accuracy: 0.8250 - loss: 10.1879 - val_accuracy: 0.9000 - val_loss: 10.0016\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step - accuracy: 0.7875 - loss: 10.1506 - val_accuracy: 0.9000 - val_loss: 10.0008\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step - accuracy: 0.8125 - loss: 10.1919 - val_accuracy: 0.9000 - val_loss: 9.9998\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 438ms/step - accuracy: 0.8000 - loss: 10.1379 - val_accuracy: 0.9000 - val_loss: 9.9985\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446ms/step - accuracy: 0.7750 - loss: 10.2112 - val_accuracy: 0.9000 - val_loss: 9.9974\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - accuracy: 0.7875 - loss: 10.1297 - val_accuracy: 0.9000 - val_loss: 9.9966\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384ms/step - accuracy: 0.8000 - loss: 10.1684 - val_accuracy: 0.9000 - val_loss: 9.9961\n",
      "\n",
      "============================================================\n",
      "BEST MODEL EVALUATION\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 9.9961\n",
      "Test Accuracy: 0.9000\n",
      "\n",
      "âœ… Best model saved as 'best_rnn_model.h5'\n"
     ]
    }
   ],
   "source": [
    "# ===== PRINT BEST HYPERPARAMETERS =====\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST HYPERPARAMETERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nğŸ“ Embedding Configuration:\")\n",
    "print(f\"  Embedding Dimension: {best_hp.get('embedding_dim')}\")\n",
    "\n",
    "print(\"\\nğŸ“Š Optimizer Configuration:\")\n",
    "print(f\"  Optimizer: {best_hp.get('optimizer')}\")\n",
    "print(f\"  Learning Rate: {best_hp.get('learning_rate'):.6f}\")\n",
    "\n",
    "print(\"\\nğŸ”„ RNN Architecture:\")\n",
    "print(f\"  Number of RNN Layers: {best_hp.get('num_rnn_layers')}\")\n",
    "\n",
    "for i in range(best_hp.get(\"num_rnn_layers\")):\n",
    "    print(f\"\\n  RNN Layer {i+1}:\")\n",
    "    print(f\"    Units: {best_hp.get(f'rnn_units_{i}')}\")\n",
    "    print(f\"    Activation: {best_hp.get(f'rnn_activation_{i}')}\")\n",
    "    print(f\"    Dropout: {best_hp.get(f'rnn_dropout_{i}'):.3f}\")\n",
    "    print(f\"    Recurrent Dropout: {best_hp.get(f'rnn_recurrent_dropout_{i}'):.3f}\")\n",
    "    print(f\"    L2 Regularization: {best_hp.get(f'rnn_l2_{i}'):.4f}\")\n",
    "\n",
    "print(\"\\nğŸ”— Dense Layers:\")\n",
    "if best_hp.get(\"use_dense_layers\"):\n",
    "    print(f\"  Number of Dense Layers: {best_hp.get('num_dense_layers')}\")\n",
    "    for i in range(best_hp.get(\"num_dense_layers\")):\n",
    "        print(f\"\\n  Dense Layer {i+1}:\")\n",
    "        print(f\"    Units: {best_hp.get(f'dense_units_{i}')}\")\n",
    "        print(f\"    Activation: {best_hp.get(f'dense_activation_{i}')}\")\n",
    "        print(f\"    Dropout: {best_hp.get(f'dense_dropout_{i}'):.3f}\")\n",
    "        print(f\"    L2 Regularization: {best_hp.get(f'dense_l2_{i}'):.4f}\")\n",
    "else:\n",
    "    print(\"  No additional dense layers used\")\n",
    "\n",
    "print(\"\\nâš™ï¸ Training Configuration:\")\n",
    "print(f\"  Batch Size: {best_hp.get('batch_size')}\")\n",
    "print(f\"  Epochs: {best_hp.get('epochs')}\")\n",
    "\n",
    "# ===== BUILD AND EVALUATE BEST MODEL (FIXED) =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BUILDING BEST MODEL FROM HYPERPARAMETERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build model with best hyperparameters instead of loading\n",
    "best_model = hypermodel.build(best_hp)\n",
    "best_model.summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING BEST MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train the best model\n",
    "history = best_model.fit(\n",
    "    x_train, \n",
    "    y_train,\n",
    "    batch_size=best_hp.get('batch_size'),\n",
    "    epochs=best_hp.get('epochs'),\n",
    "    validation_data=(x_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "test_loss, test_accuracy = best_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Optional: Save the final model\n",
    "best_model.save('best_rnn_model.h5')\n",
    "print(\"\\nâœ… Best model saved as 'best_rnn_model.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5292c789-5b18-4379-a6b3-c341931b1822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "Predicted probabilities:\n",
      " [[0.15002075]\n",
      " [0.21642639]\n",
      " [0.19165233]\n",
      " [0.16032076]\n",
      " [0.15633705]\n",
      " [0.24003525]\n",
      " [0.19619317]\n",
      " [0.1932541 ]\n",
      " [0.1810203 ]\n",
      " [0.18481408]]\n",
      "Predicted classes:\n",
      " [[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "âœ… Test Accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ===== 1. Predict probabilities =====\n",
    "y_pred_prob = best_model.predict(x_test)  # shape: (num_samples, 1)\n",
    "\n",
    "# ===== 2. Convert probabilities to class labels =====\n",
    "y_pred_class = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "# ===== 3. Print first 10 predictions =====\n",
    "print(\"Predicted probabilities:\\n\", y_pred_prob[:10])\n",
    "print(\"Predicted classes:\\n\", y_pred_class[:10])\n",
    "\n",
    "# ===== 4. Calculate accuracy =====\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "print(f\"\\nâœ… Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e886fdb-58a0-4c53-be9e-d52c5ad15e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.activations import hard_swish\n",
    "\n",
    "# Map custom function\n",
    "custom_objects = {'hard_swish': hard_swish}\n",
    "\n",
    "# Load model\n",
    "best_model = load_model('best_rnn_model.h5', compile=False, custom_objects=custom_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5cb1dbb-c95c-4b36-bf81-87dea3217ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 422ms/step\n",
      "Predicted probability: 1.0000\n",
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "line = \"go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\"\n",
    "seq = tokenizer.texts_to_sequences([line])\n",
    "\n",
    "TIMESTEPS = 20\n",
    "seq_padded = pad_sequences(seq, maxlen=TIMESTEPS, padding='post', truncating='post')\n",
    "\n",
    "# Predict\n",
    "y_pred_prob = best_model.predict(seq_padded)\n",
    "y_pred_class = int(y_pred_prob[0][0] >= 0.5)\n",
    "\n",
    "print(f\"Predicted probability: {y_pred_prob[0][0]:.4f}\")\n",
    "print(f\"Predicted class: {y_pred_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1d519-3177-468e-b466-c0d5067b4c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
